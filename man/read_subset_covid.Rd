% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/read_subset_covid.R
\name{read_subset_covid}
\alias{read_subset_covid}
\title{Read a subset of a COVerAGE-DB dataset}
\usage{
read_subset_covid(
  zippath,
  data = c("inputDB", "Output_5", "Output_10", "qualityMetrics"),
  return = c("data.frame", "data.table", "tibble"),
  Country,
  Region,
  Sex,
  Date
)
}
\arguments{
\item{zippath}{Character. The local zip archive of the downloaded dataset.}

\item{data}{The name of the dataset that is to be read. Can be one of the
the following: "inputDB", "Output_5", "Output_10", "qualityMetrics".}

\item{return}{What should be the return type? Can be on of the following:
"data.frame", "data.table", "tibble".}

\item{Country}{Character vector of countries to select.}

\item{Region}{Character vector of regions to select.}

\item{Sex}{Character vector of sexes to select. Usually either 'b' for both.
'f' for females, and 'm' for males.}

\item{Date}{Either a character or Date vector of dates to include. If a
character vector.}
}
\value{
By default a data frame with the subsetted dataset.
Can be set to return either a data table or
a tibble. The return type is controlled by the 'return' parameter.
}
\description{
Lazily reads a subset of a locally saved COVerAGE-DB dataset.
Data manipulation is handled lazily.
}
\details{
The function uses \pkg{vroom} as backend for reading the dataset
lazily. The result of this is a memory efficient processing of the data.
This approach tends to be slower than reading the whole dataset in memory.
Nevertheless, the cost in speed is advantageous to be able to work with
very large datasets, or to generally conserve memory the \emph{R} process uses.
Specifically, this is useful for reading the 'inputDB' dataset,
which currently holds millions of rows.
}
\author{
Erez Shomron
}
